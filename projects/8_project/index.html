<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Random and sparse attention for GNNs | Ghulam Murtaza </title> <meta name="author" content="Ghulam Murtaza"> <meta name="description" content="Exploring Random and Sparse Attention Mechanisms for GNNs"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gmurtaza404.github.io/projects/8_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ghulam Murtaza </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Random and sparse attention for GNNs</h1> <p class="post-description">Exploring Random and Sparse Attention Mechanisms for GNNs</p> </header> <article> <p>Many application domains of neural networks involve graph structured data, such as social networks, molecular interaction networks and gene regulatory networks, which tend to involve large numbers of nodes and connections. For example, social networks might contain millions of users of a social media website, or gene regulatory networks tends to include tens of thousands of nodes, representing genes. The advent of graph neural networks allowed users to develop deep learning models that capture the underlying information in graph topology. Although many different mechanisms have been suggested, the broad class of neighborhood aggregation has proven to be a flexible class of algorithms.</p> <p>Among these methods, Graph Attention Networks (GAT) showed state-of-the-art performance in a wide range of graph classification and node classification tasks. GAT uses an attention mechanism to calculate edge weights for each layer to adaptively scale the neighbors’ contributions based on their node features. Moreover, GAT also allows multi-headed attention to compute multiple sets of attention coefficients (edge-weights), enabling the model to have additional expressive power. However, computing attention along every edge adds to the computational cost of operations, and can make scaling GATs to large graphs infeasible.</p> <p>A subsequent work, introducing Sparse Graph Attention Networks (SGAT), shows that having different parameters to learn multi-headed attention has limited utility because most learned coefficients are redundant and dramatically increase the risk of overfitting. SGAT attempted to alleviate this problem by learning a single set of edge attention coefficients across all the layers while also learning a mask to drop relatively less important or redundant attention scores. The added sparsity constraint makes sure to greatly reduce the number of edges, and the edges to be removed are learned to simultaneously maximize the predictive accuracy. Even though, SGAT works with a significantly smaller set of edges, it still suffers from similar runtime and memory issues due to still computing all the attention scores and having to learn additional parameters for the mask.</p> <p>In language model domain, recent works Synthesizer and Big Bird attempt to address a similar problem, and improve the scalability of the attention mechanism of transformers to longer sequences. These models, among other techniques, explored random attention as a potential method to capture long range interactions without attending to the entire image, significantly reducing the memory and runtime complexity. The motivation behind this approach is that over many successive transformer layer, information can likely propagate through a random path between any two tokens even if they don’t directly attend to each other. In this project we explore a similar class of mechanisms to improve the memory and runtime complexity of the GAT by employing various methods to prune out edges from the graph. We reason that this may improve the runtime and memory complexity of Graph Attention without hurting the performance, since even if attention is not explicitly calculated along an edge, information can still flow between the two nodes through their shared neighbors. Specifically, we explore a randomized edge dropping method (GRAT), a node similarity-based edge pruning method (GCAT), and a node neighborhood-based similarity pruning method (GJAT). The goal of our analysis is to explore how a non-learning mechanism for sparsification can help GAT models scale better to larger inputs. We hypothesized that randomly sparsifying attention might have an even smaller performance trade-off for graph neural networks due to the added information present in the graph topology. Through our experiments on four benchmark datasets, we discover that for disassortative graphs, similarity-based sparsification mechanisms perform competitively while significantly reducing runtime, however, for assortative graphs, performance drops quickly with edges removed.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/random_attention_GAT-480.webp 480w,/assets/img/random_attention_GAT-800.webp 800w,/assets/img/random_attention_GAT-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/random_attention_GAT.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="memory utilization" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Predictive accuracy of each method across the four datasets. Due to class imbalance, we replace the percent accuracy metric with F-1 score in the PPI dataset. Due to compute constraints, we were unable to compute results for the SGAT model on the PPI dataset in time (PPI is significantly larger than the other datasets). GraphSAGE is designed for inductive tasks and thus could not be run on the transductive tasks. Overall, our similarity-based attention sparsification mechanisms perform competitively on disassortative graphs, yielding the best result on the Actor dataset, and a close second on the Cornell dataset. </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ghulam Murtaza. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 03, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>